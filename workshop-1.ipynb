{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB6tYmdQciGS",
    "tags": []
   },
   "source": [
    "# 1. Fundamentals\n",
    "\n",
    "Taught by: Dat Doan, Alex Ganose\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Welcome to our first practical session! If you haven't used Jyupyter Book before or your Python is getting a little rusty, make sure you complete the [Python Refresher notebook](workshop-0.ipynb) before continuing.\n",
    "\n",
    "The weekly notebooks are designed to be run online directly in your browser. You can activate the server by clicking the rocket icon on the top right and selecting `Live Code`.\n",
    "\n",
    "## Outline\n",
    "\n",
    "This workshop will cover the following content:\n",
    "\n",
    "1. Linear regression and error metrics\n",
    "1. Polynomial regression and over/underfitting\n",
    "1. Logistic and softmax regression\n",
    "1. *Advanced Topic*: Solving linear regression coefficients\n",
    "1. Task: Regress the periodic table\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Linear regression is the simplest and most common form of regression. We'll explore more powerful forms of function fitting later in the course. There is no need to code linear regression from scratch as there is an optimised [class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) already developed in the [scikit-learn](https://scikit-learn.org) package. The availability of well-developed libraries such as this are a major motiviation for using Python in data analytics.\n",
    "\n",
    "In linear regression, the response variable is a linear function of the input parameters:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x\n",
    "$$\n",
    "\n",
    "Let's start by generating some random data we can use for fitting. We can store them in a Pandas DataFrame for easy manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_points = 10 \n",
    "\n",
    "x = np.random.rand(num_points)\n",
    "y = 4 + 5 * x + np.random.randn(num_points)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "You need to choose the number of points. 10 should be fine, but you have the power to decide.\n",
    "</details>\n",
    "\n",
    "Let's plot the data using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(3, 11),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a linear regression model using scikit-learn. We will use the `LinearRegression` class from the `linear_model` module. The `fit` method is used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df.x.values.reshape(-1, 1)  # sklearn expects the features to be a 2D array\n",
    "y = df.y.values\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the fitted model parameters using the `intercept_` ($\\beta_0$) and `coef_` ($\\beta_1$) attributes of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Slope:', model.coef_)\n",
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the line of best fit on top of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot([0, 1], [model.intercept_, model.intercept_ + model.coef_[0]], color='red')\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(3, 11),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the trained model to make predictions on new data using the `predict` function. The `fit` and `predict` function names will be the same for all scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the residuals and calculate error metrics for our model. Here, we will just calculate the mean absolute error (MAE). Other error metrics will be covered in workshop 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "# note, MAE is just as easily calculated using numpy too\n",
    "mae = np.mean(np.abs(y - y_pred))\n",
    "\n",
    "print(f'Mean Absolute Error: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a final graph showing the interpretation of MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x.repeat(2).reshape(-1, 2).T, np.c_[y, y_pred].T, c=\"grey\", zorder=-1)\n",
    "ax.plot([0, 1], [model.intercept_, model.intercept_ + model.coef_[0]], color='red')\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(3, 11),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "Linear regression is only suitable for linear problems. These only makeup a small fraction of the chemical problems you might be interested in.\n",
    "\n",
    "One simple generalisation of linear regression is polynomial regression. Here the response variable depends on the powers of the input variables as\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x + \\cdots = \\phi(\\mathbf{x})^T\\boldsymbol{\\beta}\n",
    "$$\n",
    "where $\\phi$ is the link/basis function. Many popular machine learning methods (e.g. SVMs, neural nets, decision trees, etc) can be seen as just different ways of estimating basis functions.\n",
    "\n",
    "First we generate some noisy polynomial reference data and store it in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_points = 40\n",
    "\n",
    "x = 6 * np.random.rand(num_points) - 3\n",
    "y = 0.5 * x ** 2 + x + 2 + np.random.randn(num_points)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "You need to choose the number of points. 50 should be fine, but you have the power to decide.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(-3.5, 3.5),\n",
    "    ylim=(-1.5, 11),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn doesn't have a `PolynomalRegression` class. Instead, we have to create powers of the features of manually and use these with `LinearRegression`.\n",
    "\n",
    "To do this we can use the `PolynomialFeatures` class from the `preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = df.x.values.reshape(-1, 1)  # sklearn expects the features to be a 2D array\n",
    "\n",
    "# here we do not include the bias in the polynomial features since this will\n",
    "# be added automatically by the linear regression model\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# we can now transform our features to include the polynomial features\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the first column of `X_poly` is the original feature, $x$, and the second column is $x^2$.\n",
    "We can now train our model using the polynomial features. Note that we are now performing multivariate linear regression since we have more than one feature per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "print('Slope:', model.coef_)\n",
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the coefficients compare to the true values of the polynomial? You can check what these were when we defined the noisy training data.\n",
    "\n",
    "We can plot the fitted polynomial against our training data and calculate the mean absolute error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict across the range of x values\n",
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = model.predict(X_new_poly)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X_new, y_new, c=\"red\")\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(-3.5, 3.5),\n",
    "    ylim=(-1.5, 11),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# calculate the mean absolute error\n",
    "y_pred = model.predict(X_poly)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "print(f'Mean Absolute Error: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the degree of the polynomial, the better the model will fit the data. However, this can lead to overfitting, where the model is too closely tailored to the training data and does not generalize well to new, unseen data.\n",
    "\n",
    "This is why it is important to evaluate the model on a separate test set to ensure that it is not overfitting. This will be discussed more in Workshop 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for degree in (1, 2, 20):\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    y_new = model.predict(poly_features.transform(X_new))\n",
    "    ax.plot(X_new, y_new, label=f'degree={degree}')\n",
    "\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(-3.5, 3.5),\n",
    "    ylim=(-1.5, 11),\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree=1 model is underfit to the data while the degree=20 is overfit.\n",
    "\n",
    "What happens if you set the maximum degree to 100? What do you think is happening here?\n",
    "\n",
    "<details>\n",
    "<summary> Hint </summary>\n",
    "With very high polynomial degrees, you run into an issue called an explosion of terms. The large number of variables means each is given a very small weight (see the model coefficients). This can run into numerical stability issues. One way to avoid this problem is to scale the input features. This is a good idea in general for linear regression, but it is particularly important when using polynomial features.\n",
    "\n",
    "You can achieve this using the `StandardScalar` class from the `preprocessing` module.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=100, include_bias=False)\n",
    "scaler = StandardScaler()\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled, y)\n",
    "y_new = model.predict(scaler.transform(poly_features.transform(X_new)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(X_new, y_new, label=f'degree={degree}')\n",
    "ax.set(xlabel='$x$', ylabel=\"$y$\", xlim=(-3.5, 3.5), ylim=(-1.5, 11))\n",
    "```\n",
    "</details>\n",
    "\n",
    "## Logisitic Regression\n",
    "\n",
    "So far we have focussed on linear and non-linear regression which are suitable when predicting continuous response variables. For classification tasks, these approaches do not give realisitic results.\n",
    "\n",
    "Instead, one of the simplest approaches for tackling these problems is logisitic regression. Here, we use sigmoid function as the basis, giving\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\frac{e^{(\\beta_0 + \\beta_1 x)}}{1+e^{(\\beta_0 + \\beta_1 x)}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n",
    "$$\n",
    "\n",
    "We can plot this function using matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, beta0=0, beta1=1):\n",
    "    return 1 / (1 + np.exp(-beta0 - beta1 * x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "for beta1 in (1, 2, 6):\n",
    "    ax.plot(x, sigmoid(x, beta1=beta1), label=f'$\\\\beta_1={beta1}$')\n",
    "\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=r\"$\\phi(x)$\",\n",
    "    xlim=(-6, 6),\n",
    "    ylim=(-0.1, 1.1),\n",
    ")\n",
    "ax.legend()\n",
    "ax.axhline(0, c='grey', ls='--')\n",
    "ax.axhline(1, c='grey', ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply logistic regression to the iris dataset from sci-kit learn. \n",
    "This dataset contains 150 samples of iris flowers, each with four features. The target variable is the species of iris, which can be one of three possible values: setosa, versicolor, or virginica (given as integers 0, 1, and 2, respectively).\n",
    "\n",
    "First, we load the dataset and create a pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(\n",
    "    data=np.c_[iris['data'], iris['target']],\n",
    "    columns=iris['feature_names'] + ['target'],\n",
    ")\n",
    "df.target = df.target.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the dataset. What are the features and what ranges do they have? What is the target variable and what are the possible values it can take?\n",
    "\n",
    "Let's now train a logistic regression model on the dataset. To begin with, we'll only using the petal length feature to predict the target (univariate regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df[[\"petal width (cm)\"]].values.reshape(-1, 1)\n",
    "y = (df.target == 2).astype(int)  # 1 if Iris virginica, else 0\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the fitted model parameters for LogisticRegression in the same way as for LinearRegression. The naming is slightly confusing:\n",
    "- `intercept_` ($\\beta_0$): the log odds when the $x$ variable is 0.\n",
    "- `coef_` ($\\beta_1$): how much the log odds change with an increase (or decrease) in X by 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('β₀:', model.coef_)\n",
    "print('β₁:', model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a visual intepretation of the results, we can plot the sigmoid function along with the data points and the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_prob = model.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_prob[:, 1] >= 0.5][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, c=y, cmap='tab10', vmax=10)\n",
    "ax.axvline(decision_boundary, color='grey', ls=\"--\")\n",
    "ax.plot(X_new, y_prob, label=['Not Iris virginica', 'Iris virginica'])\n",
    "ax.text(decision_boundary + 0.1, 0.5, 'Decision  boundary', ha='left')\n",
    "ax.legend(loc=\"center left\")\n",
    "ax.set(\n",
    "    xlabel='Petal width (cm)', \n",
    "    ylabel='Probability', \n",
    "    xlim=[0, 3], \n",
    "    ylim=[-0.02, 1.02]\n",
    ") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the decision boundary for our logistic regression model?\n",
    "\n",
    "For classiciation problems, we can't use MAE as an error metric. Instead, we'll use  log loss (otherwise called cross-entropy loss). This will be introduced in Workshop 2.\n",
    "\n",
    "We can calculate the loss for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "loss = log_loss(y, y_pred)\n",
    "print(f'Log loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try improving our model by including more features. Let's extend the model to include both petal width and petal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "model = LogisticRegression(solver = 'lbfgs', C = 10**10, random_state = 42)\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "loss = log_loss(y, y_pred)\n",
    "print(f'Log loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss has decreased!\n",
    "\n",
    "We can plot the decision boundary in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*X.T, c=y, cmap='tab10', vmax=10)\n",
    "ax.axline((0, -model.intercept_[0] / model.coef_[0][1]), slope=-model.coef_[0][0] / model.coef_[0][1], color='grey', ls='--')\n",
    "ax.text(3.5, 1.5, 'Not Iris virginica', color='C0', ha='center')\n",
    "ax.text(6.5, 2.3, 'Iris virginica', color='C1', ha='center')\n",
    "ax.set(\n",
    "    xlabel='Petal length (cm)',\n",
    "    ylabel='Petal width (cm)',\n",
    "    xlim=[2.9, 7],\n",
    "    ylim=[0.8, 2.7]\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "\n",
    "We can further generalise logistic regression to support multiple classes by using softmax regression (also known as multinomial logistic regression). In this case, the model predicts the probability of each class and the class with the highest probability is selected as the predicted class.\n",
    "\n",
    "The general concept is as follows: for any $x$, compute a score $s_k(x)$ for each class $k$ and then convert these scores to probabilities ($\\hat{p}_k$) using the softmax function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_k(x) &= \\mathbf{x}^T \\mathbf{\\beta}_k \\\\\n",
    "\\hat p_k &= \\sigma(s(x))_k = \\frac{exp(s_k(x))}{\\Sigma_{j=1}^K exp(s_j(x))}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now use the full target value of the iris dataset, that can take values of 0, 1, and 2. The model will automatically detect that this is a multinomial problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = df.target\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a large number of points that cover the full space\n",
    "x0, x1 = np.meshgrid(\n",
    "    np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "    np.linspace(0, 3.5, 500).reshape(-1, 1),\n",
    ")\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_predict = model.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*X.T, c=y, cmap='tab10', vmax=10)\n",
    "ax.contourf(x0, x1, zz * 2 + 1.5, cmap=\"tab20\", vmax=20, vmin=0, zorder=-1)\n",
    "ax.set(\n",
    "    xlabel='Petal length (cm)',\n",
    "    ylabel='Petal width (cm)',\n",
    "    xlim=[0.5, 7.5],\n",
    "    ylim=[0, 2.6]\n",
    ")\n",
    "ax.text(1.5, 0.8, 'Setosa', color='C0', ha='center')\n",
    "ax.text(4, 0.8, 'Versicolor', color='C1', ha='center')\n",
    "ax.text(6.5, 1.5, 'Virginica', color='C2', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic: Solving linear regression coefficients\n",
    "\n",
    "The analytical solution to the linear regression problem is given by the following formula:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{X}$ is the matrix of features for all samples. This is called the normal equation. Keep in mind that for many models, we can't derive analytical solutions. In those cases we need to make use of approximations as we will see further below. \n",
    "\n",
    "Let's calculate the coefficients for the simple linear regression example we used earlier. In order to do this, we need to include a column of ones in the feature matrix $\\mathbf{X}$ to account for the intercept term. For example, our feature matrix $\\mathbf{X}$ will look like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, the coefficient vector $\\boldsymbol{\\beta}$ will look like this:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, let's create some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "x = np.random.rand(num_points)\n",
    "y = 4 + 5 * x + np.random.randn(num_points)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(-0.1, 1.1),\n",
    "    ylim=(2.5, 11.5),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the constant term to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x0 = 1 to each data point\n",
    "X_b = np.stack([np.ones(num_points), x], axis=1)\n",
    "X_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can solve for the optimal values of $\\beta$ using the normal equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print('β₀:', beta[0])\n",
    "print('β₁:', beta[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot([0, 1], [beta[0], beta[0] + beta[1]], color='red')\n",
    "ax.set(\n",
    "    xlabel='$x$',\n",
    "    ylabel=\"$y$\",\n",
    "    xlim=(-0.1, 1.1),\n",
    "    ylim=(2.5, 11.5),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIia0_h9ciGa"
   },
   "source": [
    "## Task: Regressing the periodic table\n",
    "\n",
    "For this exercise, we're going to use a periodic table data set which has been uploaded to [GitHub](https://raw.githubusercontent.com/utf/DataAnalyticsChemistry/blob/main/datasets/periodic-table.csv). You should use the skills developed in this workshop to:\n",
    "- Explore the data. What does each field mean? Plot some of the numeric data.\n",
    "- Try a linear regression on atomic number and atomic weight.\n",
    "- Can a softmax regression predict the phase at room temperature from the melting and boiling points.\n",
    "- Think of any other questions you may be able to answer through linear or logistic regression\n",
    "\n",
    "You should use your creativity to come up with novel approaches to understanding the dataset.\n",
    "\n",
    "You can use pandas to load a dataframe directly from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/utf/DataAnalyticsChemistry/refs/heads/main/datasets/periodic-table.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZfqPL6zifCa",
    "tags": []
   },
   "source": [
    "## Additional reading\n",
    "\n",
    "This notebook can be complimented with Chapter 1 of [Introduction to Statistical Learning](https://www.statlearning.com) for more background to the field."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
