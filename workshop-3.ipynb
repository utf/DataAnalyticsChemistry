{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB6tYmdQciGS",
    "tags": []
   },
   "source": [
    "# 3. Unsupervised Learning\n",
    "\n",
    "Taught by: Dat Doan, Alex Ganose\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Welcome to the second practical session! As always, the notebook is designed to be run online directly in your browser by clicking the rocket icon on the top right and selecting `Live Code`.\n",
    "\n",
    "## Outline\n",
    "\n",
    "This workshop will cover the following content:\n",
    "\n",
    "1. Principle component analysis (PCA) \n",
    "1. PCA for dimensionality reduction\n",
    "1. PCA for compression\n",
    "1. k-means clustering\n",
    "1. DB Scan\n",
    "3. Task – Exploring the ChEMBL small molecule database\n",
    "\n",
    "## Dimensionality reduction\n",
    "\n",
    "Many datasets have a large number of features which can make it slow to train models and difficult to visualize the data.\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. \n",
    "This can help to simplify the data and make it easier to analyse.\n",
    "For example, reducing the number of features can make it easier to visualize the data in two or three dimensions.\n",
    "\n",
    "Sometimes dimensionality reduction can also help to improve the performance of machine learning models by reducing the risk of overfitting.\n",
    "However, it is important to be careful when using dimensionality reduction, as it can also lead to loss of information and decrease the performance of your model.\n",
    "\n",
    "**The curse of dimensionality**\n",
    "\n",
    "The curse of dimensionality refers to the fact that as the number of features in a dataset increases, the amount of data required to cover the feature space grows exponentially.\n",
    "The table below gives the average distance between two points in a unit hypercube as a function of the number of dimensions.\n",
    "\n",
    "| number of dimensions | average distance |\n",
    "| --- | --- |\n",
    "| 2 | 0.52 |\n",
    "| 3 | 0.66 |\n",
    "| 8 | 1.13 |\n",
    "| 100 | ~4.08 |\n",
    "| 1,000 | ~12.9 | \n",
    "| 1,000,000 | ~408.25 | \n",
    "\n",
    "As the number of dimensions increases, the average distance between two points in the feature space grows.\n",
    "This means that the data becomes more sparse, and the risk of overfitting increases.\n",
    "Predictions are also much more unreliable, as new instances may be very far away from the training data set, so the model has to **extrapolate**. As the number of dimensions grows, the amount of data we need to generalise accurately grows exponentially.\n",
    "\n",
    "**Main approaches for dimensionality reduction**\n",
    "\n",
    "There are two main approaches for dimensionality reduction: \n",
    "The first is projecting the data onto a plane in lower dimensional space, called a **Projection**.\n",
    "The approach is called **Manifold Learning**. \n",
    "\n",
    "Both approaches can be visualised through the swiss roll. \n",
    "See the below code for an example. \n",
    "The first plot shows the 3D swiss roll dataset, the second shows a simple projection onto the plane spanning $x_1$ and $x_2$ and the third plot shows the 2D manifold space, essentially *unrolling* the swiss roll.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.viridis)\n",
    "ax1.view_init(10, -70)\n",
    "ax1.set(xlabel='$x_1$', ylabel='$x_2$', zlabel='$x_3$')\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.viridis)\n",
    "ax2.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(t, X[:, 1], c=t, cmap=plt.cm.viridis)\n",
    "ax3.set(xlabel='$z_1$', ylabel='$z_2$')\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see that projecting the swiss roll onto the $x_1$-$x_2$ plane would not be a good idea, as the data would be all mixed up. However, projecting it onto the $z_1$-$z_2$ plane (i.e., the unfolded plane) would be a good idea, as the data would be linearly separable.\n",
    "\n",
    "## Principle component analysis (PCA)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Singular-Value-Decomposition.svg/1024px-Singular-Value-Decomposition.svg.png\" alt=\"SVD\" width=\"30%\" style=\"float:right; margin-left=2%\">  \n",
    "\n",
    "PCA is by far the most popular technique for dimensionality reduction.\n",
    "It is a form of **projection**.\n",
    "It works by finding the directions in which the data varies the most, called the **principal components**.\n",
    "These directions are the eigenvectors of the covariance matrix of the data.\n",
    "The principal components are ordered by the amount of variance they explain in the data.\n",
    "The first principal component explains the most variance, the second explains the second most, and so on.\n",
    "You can think of PCA as choosing the best hyperplane that minimizes the mean squared distance between the original dataset and its projection onto that plane.\n",
    "\n",
    "One approach to PCA is through singular value decomposition (SVD).\n",
    "The SVD decomposes the data matrix $X$ into the dot product of three matrices: $X = U\\Sigma V^T$. \n",
    "The columns of $U$ are the principal components of $X$, and the columns of $V$ are the principal components of $X^T$.\n",
    "The diagonal elements of $\\sigma$ are the singular values of $X$.\n",
    "\n",
    "$$\n",
    "\\mathbf{V}^T =\n",
    "\\begin{pmatrix}\n",
    "  \\mid & \\mid & & \\mid \\\\\n",
    "  \\mathbf{c_1} & \\mathbf{c_2} & \\cdots & \\mathbf{c_n} \\\\\n",
    "  \\mid & \\mid & & \\mid\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $c_i$ represents the $i$-th principal component, all of which are orthogonal to each other.\n",
    "- $c_1$ is the axis that accounts for the largest amount of variance. \n",
    "- $c_2$ accounts for the largest amount of the remaining variance, etc...\n",
    "\n",
    "**Important:** PCA assumes that your data is centered! In scikit-learn this is taken care of automatically. \n",
    "Furthermore, if your features vary wildly in terms of variance, you should *scale* your data. \n",
    "\n",
    "To get a feel for PCA, let's trial it on a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 200\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, num_points)).T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there is a linear relationship between the x1 and x2 variables.\n",
    "In a regression problem we might be interested in predicting the the y values from the x values.\n",
    "However, in unsupervised learning, we would like to learn about the *relationship* between the x and y values.\n",
    "\n",
    "We can apply principal component analysis by using scikit-learns `PCA` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "As with all scikit-learn estimators, you'll need to pass the feature matrix <code>X</code> to the <code>fit</code> function.\n",
    "</details>\n",
    "\n",
    "\n",
    "The fit learns some information from the data, in particular, the \"components\" and the \"explained variance\". For example, to see the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to see the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have asked for two principal components, hence we have two sets of information for each property.\n",
    "\n",
    "Let's try and understand what these numbers mean by visualising them as vectors over the input data. The `components` define the direction of the vectors while the `explained variance` defines the squared length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', xlim=(-1, 1), ylim=(-2, 2))\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    ax.annotate(\n",
    "        \"\",\n",
    "        pca.mean_ + v,\n",
    "        pca.mean_,\n",
    "        arrowprops=dict(arrowstyle='->', color=\"red\", linewidth=2, shrinkA=0, shrinkB=0),\n",
    "    )\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors are the principal components of the data. The length is an indication of how \"important\" the axis is in capturing the variance of the data in that axis.\n",
    "\n",
    "**PCA for dimensionality reduction**\n",
    "\n",
    "We can use PCA for dimensionality reduction by removing one or more of the smallest principal components. This creates a lower-dimensional projection of the data that preserves the maximum variance of the data.\n",
    "\n",
    "We can do this by reducing the number of components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has identified the best principal component. We can now transform data by projecting it onto this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "Similar to the fit funciton, you'll need to pass the feature matrix <code>X</code> to the <code>transform</code> function.\n",
    "</details>\n",
    "\n",
    "Lets look at the shape of our data to understand the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], label=\"original\")\n",
    "ax.scatter(X_new[:, 0], X_new[:, 1], label=\"decoded\")\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', xlim=(-1, 1), ylim=(-2, 2))\n",
    "ax.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue points are the original data, while the orange are the projected version. Clearly some information about the data has been lost. Specifically, we can no longer describe the variance along the second principal component.\n",
    "The amount of variance lost is represented the the `explained_variance_` attribute we discussed above.\n",
    "\n",
    "In this case, we have reduced the dimensionality by 50%. The hope is that the other benefits this provides is enough to outweight the downsides of this loss.\n",
    "\n",
    "**PCA for noise reduction**\n",
    "\n",
    "PCA can be used as a filtering approach for noisy data. \n",
    "The idea is that any components much larger than the effect of the noise should be unaffected by the noise. So if we reconstruct the data using largest principal components we should keep the main signal and throw away the noise. \n",
    "\n",
    "We can see how this works by using the digits data from scikit learn.\n",
    "This dataset consists of 8x8 pixel images of digits from 0 to 9.\n",
    "First, let's plot some of the noise-free data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits().data\n",
    "print(digits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1797 digit images each with 64 (8x8) features (pixels). Lets plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_digits(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add some noise and replot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = np.random.normal(digits, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digits are now noisy and more difficult to make out. Let's fit a PCA on the noisy data.\n",
    "This time, rather than picking the number of components in advance, we will specify that the projection preseves at least 50% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.50).fit(  )\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "We want to fit our model on the noisy data. You can do this with:\n",
    "\n",
    "```python\n",
    "pca = PCA(0.50).fit(noisy)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "We have reduced from 64 dimensions to 12 dimensions. We can now use this fitting to project the noisy data onto the reduced space and then transform it back to the original dataset dimensions. \n",
    "This process will loose some of the variance, as shown with the linear data example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to filter out the noise, albeit at the loss of some clarity of the images. \n",
    "This filtering property can be very useful when training supervised models on noisy data.\n",
    "\n",
    "As a final demonstration, we will fit the PCA on the digits dataset again, and this time keep the full 64 dimensions. This will enable us to explore the propotion of variance explained and reveal why 12 components was selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64).fit(noisy)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(6, 6))\n",
    "ax1.plot(pca.explained_variance_ratio_ * 100)\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "ax1.set(ylabel='Explained variance (%)')\n",
    "ax2.set(ylabel='Cumulative explained variance', xlabel='Num components')\n",
    "ax2.axhline(50, ls=\":\", c=\"grey\")\n",
    "ax2.axvline(12, ls=\":\", c=\"grey\")\n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class challenge 1\n",
    "\n",
    "We will use the scikit learn wine dataset for this challenge. The dataset contains 13 features of 178 wine samples. The target is a class from 0, 1, or 2.\n",
    "You should fit a PCA to this dataset and try to answer:\n",
    "- How much of the variance is explained by the first two components?\n",
    "- How many components are needed to explain 95% of the variance?\n",
    "- Optional: plot the first two components against each other as a scatter plot and label the points with the target class.\n",
    "\n",
    "Please note, as the features vary widly in terms of variance, you need to *scale* your data before performing the PCA:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "```\n",
    "    \n",
    "We can see a summary of the dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 2, 1, code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Answer </summary>\n",
    "We can extract this information using the following code:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=13)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print(f\"2-component explained variance: {pca.explained_variance_ratio_[:2].sum() * 100:.3f}%\")\n",
    "print(f\"95% explained variance: {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1} components\")\n",
    "\n",
    "X_pca = pca.transform(X_scaled)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(3):\n",
    "    mask = y == i\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=data.target_names[i])\n",
    "ax.set(xlabel='1st component', ylabel='2nd component')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "**Non-linear dimensionality reduction**\n",
    "\n",
    "PCA is part of the family of *Projections*. The methods below are *Manifold Learning* algorithms. For more information, see here: https://scikit-learn.org/stable/modules/manifold.html. \n",
    "\n",
    "- Locally Linear Embedding (LLE)\n",
    "- Multidimensional Scaling (MDS)\n",
    "- Isomap\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "*Care:* The code below may take some time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE, Isomap, MDS, LocallyLinearEmbedding\n",
    "\n",
    "X, t = make_swiss_roll(n_samples = 1000, noise = 0.2)\n",
    "\n",
    "methods = [\n",
    "    LocallyLinearEmbedding(n_components=2, n_neighbors=10),\n",
    "    MDS(n_components=2),\n",
    "    Isomap(n_components=2),\n",
    "    TSNE(n_components=2)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize = (15, 4))\n",
    "\n",
    "for ax, method in zip(axes, methods):\n",
    "    X_reduced = method.fit_transform(X)\n",
    "    ax.set(xlabel='$z_1$', ylabel='$z_2$', title=method.__class__.__name__)\n",
    "    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.viridis)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is a type of unsupervised learning that groups similar objects into clusters.\n",
    "The goal is to partition data into groups such that points in the same group are more similar to each other than to points in other groups.\n",
    "Clustering is a form of exploratory data analysis that can reveal hidden patterns in data.\n",
    "It is the *unsupervised learning* counterpart of *classification*. \n",
    "There is no universal definition of what a cluster is, and different clustering algorithms use different definitions.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\" alt=\"clustering\" width=\"90%\" />\n",
    "\n",
    "The most common clustering algorithms are:\n",
    "- KMeans\n",
    "- DBSCAN\n",
    "- Agglomerative (hierarchical) clustering\n",
    "- Gaussian Mixture Models\n",
    "\n",
    "**k-means clustering**\n",
    "\n",
    "k-Means is the most popular clustering algorithm. It is simple, fast, and works well in practice.\n",
    "The algorithm works as follows:\n",
    "1. Initialize the cluster centers randomly.\n",
    "2. Assign each point to the nearest cluster center.\n",
    "3. Update the cluster centers to the mean of the points assigned to the cluster.\n",
    "\n",
    "The algorithm converges when the cluster centers do not change significantly between iterations.\n",
    "The number of clusters must be specified in advance.\n",
    "The algorithm is sensitive to the initialization of the cluster centers.\n",
    "It is common to run the algorithm multiple times with different initializations and choose the best result.\n",
    "\n",
    "We will demonstrate k-means clustering on some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.9, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], s=50)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `KMeans` class in scikit learn to perform the clustering.\n",
    "In this case, we know there are 4 clusters in this case (set by the `centers` parameter in `make_blobs`) so we will use this to make the clustering easier. In practical circumstances, this value will not be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the value of the predicted clusters using the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the centroids of the 4 clusters are also available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centres = kmeans.cluster_centers_\n",
    "centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the predicted clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "ax.scatter(centres[:, 0], centres[:, 1], marker=\"x\", c='red', s=200)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', title='KMeans labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this compares to the true labels. Note, the cluster labels are arbitrary and do not necessarily correspond to the true labels, so you can ignore the specific colours and instead focus on the groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', title=\"True Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did in workshop 2, we can plot the decision boundaries using the `DecisionBoundaryDisplay` class. For k-means clustering, this always corresponds to a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/Voronoi_growth_euclidean.gif\" width=200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    kmeans, \n",
    "    X, \n",
    "    alpha=0.3,\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=\"$x_1$\",\n",
    "    ylabel=\"x_2\",\n",
    "    grid_resolution=250\n",
    ")\n",
    "scatter = ax.scatter(*X.T, c=y_pred, edgecolor=\"k\")\n",
    "ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "ax.scatter(centres[:, 0], centres[:, 1], marker=\"x\", c='red', s=200)\n",
    "plt.show()\n",
    "\n",
    "# plot kmeans on iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Means++**\n",
    "\n",
    "The `KMeans` class applies an optimized algorithm by default. To get the original K-Means algorithm (for educational purposes only), you must set `init=\"random\"`, `n_init=1`and `algorithm=\"lloyd\"`. These hyperparameters will be explained below.\n",
    "\n",
    "Instead of initializing the centroids entirely randomly, it is preferable to initialize them using the following algorithm, proposed in a [2006 paper](https://goo.gl/eNUPw6) by David Arthur and Sergei Vassilvitskii:\n",
    "- Take one centroid $c_1$, chosen uniformly at random from the dataset.\n",
    "- Take a new center $c_i$, choosing an instance $\\mathbf{x}_i$ with probability: $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ where $D(\\mathbf{x}_i)$ is the distance between the instance $\\mathbf{x}_i$ and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids.\n",
    "- Repeat the previous step until all $k$ centroids have been chosen.\n",
    "\n",
    "The rest of the K-Means++ algorithm is just regular K-Means. With this initialization, the K-Means algorithm is much less likely to converge to a suboptimal solution, so it is possible to reduce `n_init` considerably (the number of repetitions of the algorithm). Most of the time, this largely compensates for the additional complexity of the initialization process.\n",
    "\n",
    "To set the initialization to K-Means++, simply set `init=\"k-means++\"` (this is actually the default). We can check how the two compare below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_naive = KMeans(n_clusters=4, n_init=1, init=\"random\", algorithm=\"lloyd\")\n",
    "kmeans_naive.fit(X)\n",
    "y_pred_naive = kmeans_naive.predict(X)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y_pred_naive, s=50, cmap='viridis')\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "ax1.set(xlabel='$x_1$', ylabel='$x_2$', title='KMeans random init')\n",
    "ax2.set(xlabel='$x_1$', ylabel='$x_2$', title='KMeans k-means++ init')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class challenge 2\n",
    "\n",
    "In most applications, the number of clusters is not known a priori.\n",
    "One way to determine the optimal number of clusters is to use the elbow method.\n",
    "The idea is to plot the sum of squared distances from each point to the closest cluster center as a function of the number of clusters.\n",
    "The point at which the curve starts to flatten is the optimal number of clusters.\n",
    "\n",
    "The `inertia_` attribute of the KMeans object gives the sum of squared distances from each point to the closest cluster center and is therefore a measure of the quality of the clustering.\n",
    "The lower the inertia, the better the clustering.\n",
    "\n",
    "For example, on our k=4 model, we can check the performance using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your challenge is to use this attribute to assess the optimal number of clusters. You should:\n",
    "- Perform k-means clustering for the number of clusters from 1 to 10.\n",
    "- Plot the inertia vs the number of clusters.\n",
    "- Identify the optimal value according to the elbow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 2, 1, code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary> Answer </summary>\n",
    "We can complete this challenge using the following code:\n",
    "\n",
    "```python\n",
    "inertia = []\n",
    "for n_clusters in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 10), inertia, marker='o')\n",
    "ax.set(xlabel='Number of clusters', ylabel='Inertia')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The correct number of clusters in this case is 4, so the method has worked well here.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Limitations of k-means clustering**\n",
    "\n",
    "Despite its strengths, k-means clustering has several limitations. Firstly, the number of clusters must be specified in advance. Furthermore, the algorithm is sensitive to:\n",
    "- Outliers\n",
    "- The scale of the data\n",
    "- The shape of the clusters\n",
    "- The density of the clusters\n",
    "\n",
    "We can demonstrate these limitations using the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data is not spherically distributed. Lets see what happens when we trial kmeans clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', title='KMeans labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the clusters are not well-separated, and the k-means algorithm struggles to identify the two moon-shaped clusters.\n",
    "\n",
    "## DBSCAN\n",
    "\n",
    "We can address this issue by using a different clustering algorithm, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n",
    "DBSCAN is a density-based clustering algorithm that groups together points that are closely packed.\n",
    "It does not require the number of clusters to be specified in advance, and it can identify clusters of arbitrary shapes.\n",
    "\n",
    "DBSCAN works by the following algorithm:\n",
    "1. For each point, it counts the number of points within a given distance $\\varepsilon$ (eps) of the point.\n",
    "2. If a point has at least `min_samples` points within $\\varepsilon$ distance (equal to 3 in the picture), it is considered a core point.\n",
    "3. A cluster is formed by grouping together core points that are reachable (within eps distance) of each other.\n",
    "4. Points that are not core points and are not within eps distance of any core points are considered noise points.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/1280px-DBSCAN-Illustration.svg.png\" alt=\"DBSCAN\" width=\"50%\" style=\"float: right; margin-left: 3%;\"/>\n",
    "\n",
    "For example, in the image, all red points near A are core points. Points B and C are not core points themselves, but are *reachable* from some core points in A, and thus also belong to that cluster. \n",
    "Point N is neither a core instance, nor reachable from any core instance and hence it is labelled as *noise*. \n",
    "\n",
    "Let's apply DBSCAN to the moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "y_pred = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we set the `eps` parameter to 0.2 and the `min_samples` parameter to 5. The `eps` parameter controls the maximum distance between two samples for one to be considered as in the neighborhood of the other. The `min_samples` parameter specifies the number of samples in a neighborhood for a point to be considered as a core point. \n",
    "\n",
    "We also use the shortcut of calling `fit_predict` to get the predicted labels, which is equivalent to first calling `fit` followed by `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', title='DBSCAN labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps surprisingly, DBSCAN does not have a `predict()` method. Instead, we have to train a classification algorithm to do that given the components of the model. If we do this, we can plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', gamma='scale', probability=True)\n",
    "svm.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    svm, \n",
    "    X, \n",
    "    alpha=0.3,\n",
    "    plot_method=\"pcolormesh\",\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    xlabel=\"$x_1$\",\n",
    "    ylabel=\"$x_2$\",\n",
    "    grid_resolution=500\n",
    ")\n",
    "scatter = ax.scatter(*X.T, c=y_pred, edgecolor=\"k\")\n",
    "ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class challenge 3 \n",
    "\n",
    "You should apply DBSCAN to the iris dataset to see how it performs on more complex data.\n",
    "- Fit the model on the iris dataset and predict the target classes.\n",
    "- How many classes does DBSCAN predict? Does this match the original dataset?\n",
    "- Plot the last two features of the data (petal length and petal width) and indicate the predicted labels.\n",
    "- Perform a comparable plot using the actual labels. Do they agree?\n",
    "- Optional: Do the same analysis with Kmeans clustering. Which does better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 2, 1, code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIia0_h9ciGa"
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary> Answer </summary>\n",
    "We can complete this challenge using the following code:\n",
    "\n",
    "```python\n",
    "dbscan = DBSCAN()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "y_pred_dbscan = dbscan.fit_predict(X)\n",
    "y_pred_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax1.scatter(X[:, 2], X[:, 3], c=y, s=50, cmap='viridis')\n",
    "ax2.scatter(X[:, 2], X[:, 3], c=y_pred_dbscan, s=50, cmap='viridis')\n",
    "ax3.scatter(X[:, 2], X[:, 3], c=y_pred_kmeans, s=50, cmap='viridis')\n",
    "ax1.set(xlabel=data.feature_names[2], ylabel=data.feature_names[3], title='True labels')\n",
    "ax2.set(xlabel=data.feature_names[2], ylabel=data.feature_names[3], title='DBSCAN labels')\n",
    "ax3.set(xlabel=data.feature_names[2], ylabel=data.feature_names[3], title='Kmeans labels')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "It appears that K-means does better in this case!\n",
    "</details>\n",
    "\n",
    "\n",
    "## Task: Explore the ChEMBL dataset\n",
    "\n",
    "This week, we will have a look at the data contained in the [ChEMBL](https://www.ebi.ac.uk/chembl/) database. \n",
    "ChEMBL (or ChEMBLdb) is a manually curated chemical database of bioactive molecules with drug-like properties.\n",
    "It is maintained by the European Bioinformatics Institute (EBI), of the European Molecular Biology Laboratory (EMBL), based at the Wellcome Trust Genome Campus, Hinxton, UK.\n",
    "- Firstly, take some time to familiarise yourself with the database. To browse all compounds available, click [here](https://www.ebi.ac.uk/chembl/g/#browse/compounds).\n",
    "- We have already prepared a CSV file of all compounds with a molecular weight *less than 200*. This should keep the dataset reasonably small to do some exploratory analysis.\n",
    "-  This challenge is completely open-ended (as with most data exploration). Play around with the data and see which variables make sense to focus on (all numeric variables or just some?).\n",
    "- Try using dimensionality reduction to make the data more accessible to various models.\n",
    "- What happens when you cluster the data? Can you find any trends in the clusters? \n",
    "- There is no correct answer here, but some are better than others.\n",
    "\n",
    "Credit: Inspiration for this challenge was taken from [here](https://github.com/nebarlow/machine_learning_for_chem_bio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/utf/DataAnalyticsChemistry/refs/heads/main/datasets/chembl-small.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZfqPL6zifCa",
    "tags": []
   },
   "source": [
    "## Additional reading\n",
    "\n",
    "This notebook can be complimented with Chapter 12 of [Introduction to Statistical Learning](https://www.statlearning.com) for more background to unsupervised learning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
